################################################################################
################################################################################
###############################   QIIME 2   ####################################
################################################################################
################################################################################

##### SESSION INFO #####
# QIIME 2 Version: 2022.2 
# R Version: 4.2.2

#### NOTE: all the following commands are to be completed in general terminal

# Activate QIIME 2 environment
conda activate /opt/homebrew/Caskroom/miniconda/base/envs/qiime2-amplicon-2023.9

################################################################################
############################# IMPORTING DATA ###################################
################################################################################

# importing data into terminal using manifest
# to create the manifest file, go into metadata table and copy & paste the 
# columns “sample_name”, “forward-absolute-filepath”, and “reverse-absolute-filepath” 
# into a new excel file and name this file “manifest.tsv” OR “manifest.txt” 

qiime tools import \
--type 'SampleData[PairedEndSequencesWithQuality]' \
--input-path manifest.txt \ 
--output-path paired-end-demux.qza \ 
--input-format PairedEndFastqManifestPhred33V2 
#--input-format is quality control index for paired-end analysis with Illumina

qiime demux summarize \
--i-data paired-end-demux.qza \ 
--o-visualization demux.qzv 

################################################################################
################################ TRIMMING ######################################
################################################################################

################  OPTION 1: Figaro  ################  

# Download Figaro 
# This is a tool for optimizing microbiome rRNA gene trimming parameters
git clone https://github.com/Zymo-Research/figaro.git

# Move into working directory of Figaro
cd figaro
cd figaro # there's another "figaro" in the figaro working directory 
# MOVE setup.py INTO THE SECOND FIGARO FOLDER!!!

# Make sure python/pip is properly downloaded (can be python3/pip3 too)
pip3 install --upgrade pip

# You need bdist_wheel
python3 setup.py bdist_wheel
pip3 install --force-reinstall dist/*.whl

# Install python/pip
curl https://bootstrap.pypa.io/get-pip.py | python
pip install --upgrade setuptools

# install wheel, numpy, scipy, matplotlib
# use pip3 or pip
pip install wheel
conda install -c anaconda numpy 
# python3 -m pip install numpy
pip install scipy
pip install matplotlib

# Use this command to ensure that python is active, the "--help" will give a 
# Breakdown of commands to use and what they mean
python figaro.py --help

#-#-# Figaro Template Set-Up #-#-#
# This is the template for trimming
figaro -i /path/to/fastq/directory -o /path/to/output/files 
-a [amplicon length] \
-f [forward primer length] -r [reverse primer length]

# Run Figaro to determine where you need to trim your forward and reverse reads 
python figaro.py -i /path/to/fastq/directory -o /path/to/output/files 
-a 251 -f 31 -r 32

#-#-# Figaro Command #-#-# 
# Trim reads with figaro 
python3 figaro.py -i /Users/leahdaloisio/Desktop/16S-analysis/16S-data-reads 
-o /Users/leahdaloisio/Desktop/Output_Reads-2 -a 251 -f 31 -r 32
# top values chosen [129, 205]

# The output for this command will be several rows looking at the 
# "maxExpectedError" for each "trimPosition", choose the top row, which will 
# tell you the trim position for denoising 


################################################################################
################################# DENOISING ####################################
################################################################################

# Denoising step using DADA2
qiime dada2 denoise-paired \ # denoise-paired performs quality filtering,
# chimera checking, and paired-end read joining, also does stitching for you
--i-demultiplexed-seqs paired-end-demux.qza \ # use file made in import step
--p-trunc-len-f 129 \ # cut-off point for forward read determined by figaro 
--p-trunc-len-r 205 \ # cut-off point for reverse read determined by figaro
--p-trim-left-f 31 \
--p-trim-left-r 32 \
--o-table table.qza \ 
--o-representative-sequences rep-seqs.qza \
--o-denoising-stats denoising-stats.qza 


################################################################################
######################### GENERATING TAXONOMY TABLE ############################
################################################################################

# Install Greengenes2
source activate qiime2.2022.8
pip install q2-greengenes2

# Classify with Greengenes2
qiime feature-classifier classify-sklearn \
--i-classifier 2022.10.backbone.full-length.nb.qza \
--i-reads rep-seqs.qza \
--o-classification taxonomy-gg2.qza

# Create the taxonomy.qzv for visualization 
qiime metadata tabulate \
--m-input-file taxonomy-gg2.qza \
--o-visualization taxonomy-gg2.qzv

# View summary of DADA2 outputs
qiime metadata tabulate \
--m-input-file denoising-stats.qza \
--o-visualization denoising-stats-summ.qzv

# Generate a tree for phylogenetic diversity analysis
qiime phylogeny align-to-tree-mafft-fasttree \
--i-sequences rep-seqs.qza \
--o-alignment aligned-rep-seqs.qza \
--o-masked-alignment masked-aligned-rep-seqs.qza \
--o-tree unrooted-tree.qza \
--o-rooted-tree rooted-tree.qza


################################################################################
################################ FILTERING ASVs ################################ 
################################################################################

qiime feature-table summarize \            
--i-table table.qza \                               
--m-sample-metadata-file sample-metadata-full.txt \
--o-visualization table.qzv 

qiime feature-table tabulate-seqs \
--i-data rep-seqs.qza \
--o-visualization rep-seqs.qzv

# Remove the blanks and standards
qiime feature-table filter-samples \
--i-table table.qza \
--m-metadata-file sample-metadata-full.txt \
--p-where "host_sex = 'NA'" \
--p-exclude-ids \
--o-filtered-table table-filtered.qza

# Check to make sure the blanks and standards were removed
qiime feature-table summarize \
--i-table table-filtered.qza \
--m-sample-file sample-metadata-full.txt \
--o-visualization table-filtered.qzv

# Filter out taxa that are just phyla (unclassified ASVs), and mitochondria and 
# chloroplasts (contaminant ASVs)
qiime taxa filter-table \
--i-table table-filtered.qza \
--i-taxonomy taxonomy-gg2.qza \
--p-mode contains \
--p-include p__ \
--p-exclude 'p__;,Chloroplast,Mitochondria' \ 
--o-filtered-table table-filtered-taxa.qza

# Summarize to see if this filter step removed any samples
qiime feature-table summarize \
--i-table table-filtered-taxa.qza \
--m-sample-metadata-file sample-metadata_V5.txt \
--o-visualization table-filtered-taxa.qzv

# Now filter the rep.seqs.qza based on changes in the table.qza (will need this for BugBase)
qiime feature-table filter-seqs \
--i-data rep-seqs.qza \
--i-table table-filtered-taxa.qza \
--o-filtered-data rep-seqs-filtered.qza


################################################################################
########################## GENERATING QIIME2 RESULTS ###########################
################################################################################

# This command is normalizing using the rarefaction curve and 
# setting sampling depth to 13224
# You used the outputs of the bray curtis and wunifac matrices to create your beta diveristy plots in R!!!
qiime diversity core-metrics-phylogenetic \
--i-phylogeny rooted-tree.qza \
--i-table table-filtered-taxa.qza \
--p-sampling-depth 12053 \
--m-metadata-file sample-metadata_V5.txt \ 
--output-dir core-metrics-results_gg2b 


# Create taxa barplots
qiime taxa barplot \
--i-table table-filtered-taxa.qza \
--i-taxonomy taxonomy-gg2.qza \
--m-metadata-file sample-metadata_V5.txt \
--o-visualization taxa-bar-plots.qzv


################################################################################
############################ PREP FILES FOR RSTUDIO ############################
################################################################################

# To use your OTU table (table.qza) in R, you need it to be a tsv file
# STEP 1: export as biom table 
qiime tools export --input-path table-filtered-taxa.qza --output-path exported 
# this will automatically export a file called " feature-table.biom "

# STEP 2: now convert into .tsv file 
biom convert -i exported/feature-table.biom -o feature-table-gg2.tsv --to-tsv # run just this line to get .tsv
# manually open file and delete out extra header on top row

# Also need to export taxonomy table
qiime tools export --input-path taxonomy.qza --output-path exported
# this will automatically export a file called " taxonomy.tsv "


################################################################################
################################   R STUDIO    #################################
################################################################################


# load required packages 
library(BiocManager)
library(phyloseq)
library(ANCOMBC)
library(DESeq2)
library(tidyverse)
library(ComplexHeatmap)
library(dplyr)
library(tidyr)
library(stringr)


setwd("/Users/leahdaloisio/Desktop/16S-analysis")

# Use feature table derived using QIIME2 to construct your OTU table in R
otu <- read.table(file = "feature-table-gg2.tsv", 
                  sep = "\t", header = T, row.names = 1, 
                  skip = 0, comment.char = "")

# use taxonomy table derived using QIIME2 to construct your taxonomy table in R
taxonomy <- read.table(file = "taxonomy-gg2.tsv", sep = "\t", 
                       header = T ,row.names = 1)

# clean the taxonomy table -- make sure the tidyverse package is loaded for this
tax <- taxonomy %>%
  select(Taxon) %>%
  separate(Taxon, c("Kingdom", "Phylum", "Class", "
                    Order", "Family", "Genus", "Species"), "; ")

tax.clean <- data.frame(row.names = row.names(tax),
                        Kingdom = str_replace(tax[,1], "k__",""),
                        Phylum = str_replace(tax[,2], "p__",""),
                        Class = str_replace(tax[,3], "c__",""),
                        Order = str_replace(tax[,4], "o__",""),
                        Family = str_replace(tax[,5], "f__",""),
                        Genus = str_replace(tax[,6], "g__",""),
                        Species = str_replace(tax[,7], "s__",""),
                        stringsAsFactors = FALSE)

tax.clean[is.na(tax.clean)] <- ""
tax.clean[tax.clean=="__"] <- ""

for (i in 1:nrow(tax.clean)){
  if (tax.clean[i,7] != ""){
    tax.clean$Species[i] <- paste(tax.clean$Genus[i], tax.clean$Species[i], 
                                  sep = " ")
  } else if (tax.clean[i,2] == ""){
    kingdom <- paste("Unclassified", tax.clean[i,1], sep = " ")
    tax.clean[i, 2:7] <- kingdom
  } else if (tax.clean[i,3] == ""){
    phylum <- paste("Unclassified", tax.clean[i,2], sep = " ")
    tax.clean[i, 3:7] <- phylum
  } else if (tax.clean[i,4] == ""){
    class <- paste("Unclassified", tax.clean[i,3], sep = " ")
    tax.clean[i, 4:7] <- class
  } else if (tax.clean[i,5] == ""){
    order <- paste("Unclassified", tax.clean[i,4], sep = " ")
    tax.clean[i, 5:7] <- order
  } else if (tax.clean[i,6] == ""){
    family <- paste("Unclassified", tax.clean[i,5], sep = " ")
    tax.clean[i, 6:7] <- family
  } else if (tax.clean[i,7] == ""){
    tax.clean$Species[i] <- paste("Unclassified ",tax.clean$Genus[i], sep = " ")
  }
}


################################################################################
############################ CREATE PHYLOSEQ FILE ##############################
################################################################################

library(phyloseq)

# Read in metadata for ps object
metadata <- read.csv("sample-metadata_V5.csv")

# Handling empty sample names by assigning them a unique identifier
metadata$sample_name[metadata$sample_name == ""] <- paste("EmptyName", seq_along(metadata$sample_name[metadata$sample_name == ""]), sep="_")

# Handling duplicates by ensuring unique sample names
metadata$sample_name <- make.unique(metadata$sample_name)

# Set row names
rownames(metadata) <- metadata$sample_name
metadata <- metadata[, !names(metadata) %in% "sample_name"]

# Convert to sample_data
SAMPLE <- sample_data(metadata)

# Read in OTU table for ps object
OTU = otu_table(as.matrix(otu), taxa_are_rows = TRUE) 

# Read in Taxonomy table for ps object
TAX = tax_table(as.matrix(tax.clean))
# Re-order the tax table to be the same order as OTU (need to have them matched to create ps)
TAX <- TAX[match(row.names(OTU), row.names(TAX)), ]

# Read in tree for ps object
TREE = read_tree("tree-gg2.nwk")

# Merge the data into a file called "ps"
ps <- phyloseq(OTU, TAX, SAMPLE, TREE)

################################################################################
########################## ALPHA DIVERSITY ANALYSIS ############################
################################################################################

library(ggplot2)
library (vegan)
library(dunn.test)

# Ensure that 'group' is a factor and set the levels in the desired order
sample_data(ps)$group <- factor(sample_data(ps)$group, levels = c("Indian", "Indo-Immigr", "Indo-Can", "Euro-Can", "Euro-Immigr"))


### Plot Shannon
plot_shannon <- plot_richness(ps, x = "group", measures = c("Shannon")) +
  geom_boxplot() +
  theme_classic() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    strip.background = element_blank(),
    strip.text.x = element_blank(),
    plot.title = element_blank()
  ) +
  labs(y = "Shannon Index", title = "")

print(plot_shannon)

### Plot Pielou's Evenness
# Calculate Shannon Index and Pielou's Evenness
richness_estimates <- estimate_richness(ps, measures = c("Shannon"), split = TRUE)
species_richness <- estimate_richness(ps, measures = c("Observed"), split = TRUE)
richness_estimates$Pielou <- richness_estimates$Shannon / log(species_richness$Observed)

# Cnvert to long format for ggplot2
df_pielou <- data.frame(sample_data(ps), Pielou = richness_estimates$Pielou)

# Now plot Pielou's Evenness
plot_pielou <- ggplot(df_pielou, aes(x = group, y = Pielou)) +
  geom_boxplot() +
  theme_classic() +
  theme(
    strip.background = element_blank(),
    axis.text.x = element_text(angle = 45, hjust = 1)
  ) +
  labs(y = "Pielou's Evenness", title = NULL)

print(plot_pielou)

## Both Shannon and Pielou's were tested with rarefied data as well, but no major
## differences were observed


################################################################################
########################### NORMALIZATION METHODS ##############################
################################################################################

########## RAREFYING ########## 

set.seed(111) # keep result reproductive
ps.rarefied = rarefy_even_depth(ps, rngseed=1, sample.size=12053, replace=F) 
?ps.rarefied
?rarefy_even_depth
#rarefying data to 12,053 sequences

ps.rarefied 

################################################################################
########################  BETA DIVERSITY WITH ANALYSIS  ########################
################################################################################

#### NOTE: Using QIIME2 results here ####

library(qiime2R)
library(ggplot2)
library(stats)  # for cmdscale function


####### Bray Curtis #######

# Import the qza file
distance_matrix_qza <- read_qza("core-metrics-results_gg2b/bray_curtis_distance_matrix.qza")

# Convert to a matrix
distance_matrix <- as.matrix(distance_matrix_qza$data)

# Compute classical MDS (equivalent to PCoA)
pcoa_result <- cmdscale(distance_matrix, eig = TRUE, k = 2)

# Convert to a data frame
pcoa_data <- as.data.frame(pcoa_result$points) %>%
  tibble::rownames_to_column("sample_name")

# Flip the Y-axis
pcoa_data$V2 <- pcoa_data$V2 * -1

# Merge metadata
metadata <- read.delim("sample-metadata_V5.txt", header = TRUE)
combined_df <- left_join(metadata, pcoa_data, by = "sample_name")

# Custom RGB colors
custom_colors <- c(
  "Euro-Can" = rgb(255/255, 215/255, 0/255),
  "Indian" = rgb(3/255, 192/255, 74/255),
  "Euro-Immigr" = rgb(0/255, 204/255, 204/255),
  "Indo-Can" = rgb(242/255, 80/255, 34/255),  
  "Indo-Immigr" = rgb(191/255, 64/255, 191/255)  
)

# Placeholder percentages of variation
percent_variation_axis1 <- "21.38%"
percent_variation_axis2 <- "7.235%"
percent_variation_axis3 <- "4.523%"

# Plot using ggplot
p <- ggplot(combined_df, aes(x = V1, y = V2, color = group)) +
  geom_point() +
  stat_ellipse(aes(fill = group), geom = "polygon", level = 0.95, alpha = 0.1) + 
  labs(
    title = "Bray-Curtis PCoA with Ellipses", 
    x = paste("PCoA 1 (", percent_variation_axis1, ")", sep = ""),
    y = paste("PCoA 2 (", percent_variation_axis2, ")", sep = "")
  ) +
  scale_color_manual(values = custom_colors) +
  scale_fill_manual(values = custom_colors) +
  theme_minimal() +
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.line = element_line(colour = "black", size = 0.5)
  )

print(p)


####### Weighted Unifrac #######

library(qiime2R)
library(tidyverse)
library(ggplot2)
library(vegan)

# Load Weighted UniFrac distance matrix
weighted_unifrac_qza <- read_qza("core-metrics-results_gg2/weighted_unifrac_distance_matrix.qza")
weighted_unifrac_matrix <- weighted_unifrac_qza$data

# PCoA analysis
pcoa_results2 <- vegan::capscale(weighted_unifrac_matrix ~ 1)

# Extract PCoA data
pcoa_data2 <- pcoa_results2$data$Vectors %>%
  as.data.frame() %>%
  tibble::rownames_to_column("sample_name")

pcoa_data2 <- as.data.frame(pcoa_results2$CA$u) %>%
  rownames_to_column(var = "sample_name") %>%
  select(sample_name, MDS1, MDS2)

# Load metadata
metadata <- read.delim("sample-metadata_V5.txt", header = TRUE)

# Join metadata with PCoA data
combined_df2 <- left_join(metadata, pcoa_data2, by = "sample_name")

# Custom RGB colors
custom_colors <- c(
  "Euro-Can" = rgb(255/255, 215/255, 0/255),
  "Indian" = rgb(3/255, 192/255, 74/255),
  "Euro-Immigr" = rgb(0/255, 204/255, 204/255),
  "Indo-Can" = rgb(242/255, 80/255, 34/255),  
  "Indo-Immigr" = rgb(191/255, 64/255, 191/255)  
)

# Placeholder percentages of variation
percent_variation_axis1 <- sprintf("%.1f%%", 100 * pcoa_results2$CA$eig[1] / sum(pcoa_results2$CA$eig))
percent_variation_axis2 <- sprintf("%.1f%%", 100 * pcoa_results2$CA$eig[2] / sum(pcoa_results2$CA$eig))

# Plot using ggplot

p2 <- ggplot(combined_df2, aes(x = MDS1, y = -MDS2, color = group)) + # negative sign before MDS2 is to just flip the points (a preference)
  geom_point() +
  stat_ellipse(aes(fill = group), geom = "polygon", level = 0.95, alpha = 0.2) +
  labs(
    title = "Weighted UniFrac PCoA with Ellipses", 
    x = paste("PCoA 1 (", percent_variation_axis1, ")", sep = ""),
    y = paste("PCoA 2 (", percent_variation_axis2, ")", sep = "")
  ) +
  scale_color_manual(values = custom_colors) +
  scale_fill_manual(values = custom_colors) +
  theme_minimal() +
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.line = element_line(colour = "black", size = 0.5)
  )


print(p2)

################################################################################
#################################  LEFSE  ######################################
################################################################################

# Script taken from QIIME2 forum: https://forum.qiime2.org/t/lefse-after-qiime2/4496/7

# Collapse the table.qza to a level (taxonomic level you want)
qiime taxa collapse \
--i-table table-filtered-taxa.qza  \
--o-collapsed-table table-L7.qza \
--p-level 7 \
--i-taxonomy taxonomy-gg2.qza

# Calculate relative frequency for the collapsed table (instead of counts you
# get relative abundance)
qiime feature-table relative-frequency \
--i-table table-L7.qza \
--o-relative-frequency-table lefse-table-L7.qza \
--output-dir lefse/
  
# Export biom file
qiime tools export \
--input-path lefse-table-L7.qza \
--output-path lefse-L7/
  
# Convert biom file to a text file (for LeFSe comparison)
biom convert \
-i lefse-L7/feature-table.biom \
-o lefse-table-L7-LAST.txt --header-key "taxonomy" --to-tsv


################################################################################
################################  LEFSE   ######################################
################################################################################

# 3.5 threshold

# one against all 
lefse_format_input.py lefse_table-4.txt lefse_table-4.in -c 1 -u 2 -o 1000000

# run lefse analysis
lefse_run.py  lefse_table-4.in  lefse_table-4.res -l 3.5
# edit the names in the .in file to make it as you'd like .. import into excel and change the info to ".in" after

# create visualization 
lefse_plot_res.py  NIJ-filtered-9.res  NIJ-filtered.png --left_space 0.4 --dpi 1200 

# create cladogram 
lefse_plot_cladogram.py NIJ-filtered.res NIJ-filtered_clad.svg --background_color w --labeled_stop_lev 7 --class_legend_font_size 4
# couldn't get the legend to appear so did it in Galaxy instead

################################################################################
##############################  RDA PLOTS   ####################################
################################################################################ 

# Reference file: RDA-2_2023-09-16.R 

################################################################################
##########################  SAMPLE SIZE CALCULATIONS  ##########################
################################################################################

############### Bray Curtis Distance for Sample size Calculation ###############

# Calculate the distance matrix
dist <- phyloseq::distance(ps.rarefied, method = "bray")

# Extract cohort information (replace "group" with your actual column name)
cohort <- sample_data(ps.rarefied)$group

# Convert distance matrix to a data frame
dist_df <- as.data.frame(as.matrix(dist))

# Add cohort information to the distance data frame
dist_df$cohort <- cohort

# Initialize vectors to store results
cohorts <- unique(cohort)
iqr_distance <- numeric(length(cohorts))
median_distance <- numeric(length(cohorts))

# Calculate median distance and IQR for each cohort
for (i in 1:length(cohorts)) {
  current_cohort <- cohorts[i]
  
  # Extract distances for pairs of samples within the current cohort
  cohort_samples <- which(cohort == current_cohort)
  cohort_distances <- dist_df[cohort_samples, cohort_samples]
  
  # Convert the upper triangle of the distance matrix to a vector
  cohort_dist_vector <- as.vector(cohort_distances[upper.tri(cohort_distances)])
  
  # Calculate median and IQR for the cohort
  median_distance[i] <- median(cohort_dist_vector, na.rm = TRUE)
  iqr_distance[i] <- IQR(cohort_dist_vector, na.rm = TRUE)
}

# Create a data frame for median and IQR results
summary_table <- data.frame(cohort = cohorts, median_distance = median_distance, IQR = iqr_distance)

# Print the results
print(summary_table)

#        cohort median_distance       IQR
# 1    Euro-Can       0.6218784 0.1154484
# 2      Indian       0.8273044 0.1767817
# 3 Indo-Immigr       0.6088526 0.1285987
# 4    Indo-Can       0.6330789 0.1206961
# 5 Euro-Immigr       0.6669709 0.1226251


## Indian vs. Euro-Canadians
# (0.8273044 - 0.6218784)  / 0.1767817
# 1.162032^2 = 1.350318
# (7.84/1.350318)*2
# = 11.6
# = 12 ppl per group

## Indian vs. Indo-Immigrants
# (0.8273044 - 0.6088526)  / 0.1767817
# 1.235715^2 = 1.526992
# (7.84/1.526992)*2
# = 10.3
# = 10 ppl per group

## Indian vs. Indo-Canadians
# (0.8273044 - 0.6330789) / 0.1767817
# 1.098674^2 = 1.207085
# (7.84/1.207085)*2
# = 12.9
# = 13 ppl per group

## Indo-Immigrants vs. Indo-Canadians
# (0.6330789 - 0.6088526) / 0.1285987
# 0.1883868^2 = 0.03548959
# (7.84/0.03548959)*2
# = 441.8
# = 442 ppl per group

## Indo-Immigrants vs. Euro-Canadians
# (0.6218784 - 0.6088526) / 0.1285987
# 0.1012903^2 = 0.01025972
# (7.84/0.01025972)*2
# = 1528.307
# = 1528 ppl per group
